{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport collections\n\nclass SentencePiece:\n    def __init__(self, vocab_size=1000):\n        self.vocab_size = vocab_size\n        self.vocab = {}  # Maps token -> log_probability\n        self.max_token_len = 5 # Reduced to 5 for small-data demo (Prevents overfitting)\n\n    def _get_all_substrings(self, text):\n        \"\"\"\n        Helper: Generates all possible substrings up to max_token_len.\n        \"\"\"\n        substring_counts = collections.defaultdict(int)\n        n = len(text)\n        \n        for i in range(n):\n            for j in range(i + 1, min(i + self.max_token_len + 1, n + 1)):\n                sub = text[i:j]\n                substring_counts[sub] += 1\n        return substring_counts\n\n    def _viterbi(self, text):\n        \"\"\"\n        The Viterbi Algorithm (Forward-DP) to find the best segmentation.\n        \"\"\"\n        n = len(text)\n        # best_score[i] = max log-probability of segmenting text[:i]\n        best_score = [-float('inf')] * (n + 1)\n        best_score[0] = 0.0\n        \n        # best_segment[i] = the subword added to reach index i\n        best_segment = [None] * (n + 1)\n        \n        for i in range(1, n + 1):\n            # Look back only as far as our longest possible token\n            for j in range(max(0, i - self.max_token_len), i): \n                sub = text[j:i]\n                if sub in self.vocab:\n                    # Score = score_at_j + log_prob(sub)\n                    score = best_score[j] + self.vocab[sub]\n                    \n                    if score > best_score[i]:\n                        best_score[i] = score\n                        best_segment[i] = sub\n                        \n        if best_score[n] == -float('inf'):\n            # Fallback: return characters if no path found (should be rare)\n            return list(text)\n            \n        # Backtracking (Reverse path reconstruction)\n        path = []\n        curr = n\n        while curr > 0:\n            sub = best_segment[curr]\n            path.append(sub)\n            curr -= len(sub)\n            \n        return path[::-1] # Reverse to get correct order\n\n    def train(self, data, epochs=5):\n        \"\"\"\n        EM Algorithm for Unigram Language Model.\n        \"\"\"\n        print(f\"--- Starting SentencePiece Training (Target Vocab: {self.vocab_size}) ---\")\n        \n        # 1. Initialization (Seed Vocab)\n        full_text = \"\".join(data)\n        substring_counts = self._get_all_substrings(full_text)\n        \n        # CRITICAL FIX: Explicitly add single characters to ensure coverage.\n        # This guarantees the Viterbi algorithm always has a valid \"fallback\" path.\n        unique_chars = set(full_text)\n        for char in unique_chars:\n            substring_counts[char] += 1\n        \n        # Calculate initial probabilities (Frequency / Total)\n        total_count = sum(substring_counts.values())\n        self.vocab = {k: math.log(v / total_count) for k, v in substring_counts.items()}\n        \n        print(f\"Initial Seed Vocab Size: {len(self.vocab)}\")\n\n        for epoch in range(epochs):\n            # --- E-Step (Expectation) ---\n            # Tokenize the entire dataset with the current best model\n            subword_counts = collections.defaultdict(int)\n            for sentence in data:\n                tokens = self._viterbi(sentence)\n                for t in tokens:\n                    subword_counts[t] += 1\n            \n            # --- M-Step (Maximization) ---\n            # Update probabilities based on usage\n            current_total = sum(subword_counts.values())\n            self.vocab = {}\n            for t, count in subword_counts.items():\n                self.vocab[t] = math.log(count / current_total)\n            \n            # --- Pruning ---\n            # Keep only the top-k most probable tokens\n            if len(self.vocab) > self.vocab_size:\n                sorted_tokens = sorted(self.vocab.items(), key=lambda x: x[1], reverse=True)\n                self.vocab = dict(sorted_tokens[:self.vocab_size])\n            \n            print(f\"Epoch {epoch+1}: Vocab size refined to {len(self.vocab)}\")\n            \n        print(\"--- Training Complete ---\")\n\n    def encode(self, text):\n        return self._viterbi(text)\n\nif __name__ == \"__main__\":\n    # Test Data\n    data = [\n        \"the quick brown fox jumps\",\n        \"the quick brown fox jumped\",\n        \"modeling unrelated concepts is difficult\",\n        \"relation between tokens is complex\",\n        \"related and unrelated words\",\n        \"tokenization is the first step\"\n    ]\n    \n    # Train\n    # We use vocab_size=50 to force it to pick the \"best\" subwords\n    sp = SentencePiece(vocab_size=50) \n    sp.train(data, epochs=5)\n    \n    print(\"\\n--- Learned Vocabulary (Top 10) ---\")\n    # Sort by probability (highest log-prob first)\n    top_tokens = sorted(sp.vocab.items(), key=lambda x: x[1], reverse=True)[:10]\n    for token, score in top_tokens:\n        print(f\"'{token}': {score:.4f}\")\n    \n    # Inference Tests\n    print(\"\\n--- Inference ---\")\n    test_words = [\"unrelated\", \"tokenization\", \"jumped\"]\n    \n    for w in test_words:\n        print(f\"'{w}' -> {sp.encode(w)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-04T06:57:44.075548Z","iopub.execute_input":"2026-02-04T06:57:44.076845Z","iopub.status.idle":"2026-02-04T06:57:44.099720Z","shell.execute_reply.started":"2026-02-04T06:57:44.076806Z","shell.execute_reply":"2026-02-04T06:57:44.098702Z"}},"outputs":[{"name":"stdout","text":"--- Starting SentencePiece Training (Target Vocab: 50) ---\nInitial Seed Vocab Size: 523\nEpoch 1: Vocab size refined to 35\nEpoch 2: Vocab size refined to 35\nEpoch 3: Vocab size refined to 35\nEpoch 4: Vocab size refined to 35\nEpoch 5: Vocab size refined to 35\n--- Training Complete ---\n\n--- Learned Vocabulary (Top 10) ---\n'the q': -2.9444\n'uick ': -2.9444\n'brown': -2.9444\n' fox ': -3.6376\n'jumps': -3.6376\n' ': -3.6376\n'fox j': -3.6376\n'umped': -3.6376\n'model': -3.6376\n'ing u': -3.6376\n\n--- Inference ---\n'unrelated' -> ['u', 'n', 'r', 'e', 'l', 'a', 't', 'e', 'd']\n'tokenization' -> ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n'jumped' -> ['j', 'u', 'm', 'p', 'e', 'd']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}