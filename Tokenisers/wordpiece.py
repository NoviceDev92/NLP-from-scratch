{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport collections\n\nclass WordPiece:\n    def __init__(self, vocab_size=200):\n        self.vocab_size = vocab_size\n        self.vocab = {}      # Maps token -> ID\n        self.inv_vocab = {}  # Maps ID -> token\n\n    def get_stats(self, vocab):\n        \"\"\"\n        Computes counts of pairs (numerator) and individual tokens (denominator).\n        vocab format: {'w ##o ##r ##d': count}\n        \"\"\"\n        pairs = collections.defaultdict(int)\n        token_freqs = collections.defaultdict(int)\n        \n        for word, freq in vocab.items():\n            symbols = word.split()\n            \n            # Count individual tokens for the denominator\n            for sym in symbols:\n                token_freqs[sym] += freq\n                \n            # Count adjacent pairs for the numerator\n            for i in range(len(symbols)-1):\n                pairs[symbols[i], symbols[i+1]] += freq\n                \n        return pairs, token_freqs\n\n    def calculate_scores(self, pairs, token_freqs):\n        \"\"\"\n        Score = Count(AB) / (Count(A) * Count(B))\n        This prioritizes 'correlated' pairs (like 'q' + 'u') over just 'frequent' ones.\n        \"\"\"\n        scores = {}\n        for (a, b), pair_count in pairs.items():\n            if token_freqs[a] == 0 or token_freqs[b] == 0:\n                scores[a, b] = 0\n            else:\n                scores[a, b] = pair_count / (token_freqs[a] * token_freqs[b])\n        return scores\n\n    def merge_vocab(self, pair, v_in):\n        v_out = {}\n        a, b = pair\n        \n        # 1. ESCAPE properly for Regex\n        pattern = re.compile(r'(?<!\\S)' + re.escape(a) + r' ' + re.escape(b) + r'(?!\\S)')\n        \n        # 2. THE FIX: Clean the merge\n        # If we merge \"u\" and \"##n\", we want \"un\", not \"u##n\"\n        # If we merge \"##i\" and \"##n\", we want \"##in\", not \"##i##n\"\n        if b.startswith(\"##\"):\n            replacement = a + b[2:] # Remove the '##' from the second part\n        else:\n            replacement = a + b\n        \n        for word in v_in:\n            w_out = pattern.sub(replacement, word)\n            v_out[w_out] = v_in[word]\n            \n        return v_out\n\n    def train(self, data):\n        \"\"\"\n        Train the WordPiece tokenizer on a list of strings (sentences or words).\n        \"\"\"\n        print(f\"--- Starting WordPiece Training (Target Vocab: {self.vocab_size}) ---\")\n        \n        # 1. Flatten sentences into words and count them\n        all_words = []\n        for sentence in data:\n            # Simple whitespace split to get words\n            all_words.extend(sentence.split())\n            \n        word_counts = collections.Counter(all_words)\n        \n        # 2. Initialize Vocab: \"word\" -> \"w ##o ##r ##d\"\n        vocab = {}\n        for w, count in word_counts.items():\n            if not w: continue\n            chars = [w[0]] + [\"##\" + c for c in w[1:]]\n            vocab[\" \".join(chars)] = count\n\n        # 3. Main Loop\n        while True:\n            # A. Get counts\n            pairs, token_freqs = self.get_stats(vocab)\n            \n            # B. Safety Check: Stop if no pairs left to merge\n            if not pairs:\n                print(\"No more pairs to merge. Stopping.\")\n                break\n            \n            # C. Compute Likelihood Scores & Pick Best\n            scores = self.calculate_scores(pairs, token_freqs)\n            best_pair = max(scores, key=scores.get)\n            \n            # D. Merge\n            old_vocab = vocab.copy() # Snapshot for safety check\n            vocab = self.merge_vocab(best_pair, vocab)\n            \n            # E. Safety Check: Stop if merge didn't change anything (infinite loop prevention)\n            if vocab == old_vocab:\n                print(f\"Stagnation detected at pair {best_pair}. Stopping.\")\n                break\n            \n            # F. Check Vocab Size\n            # We count unique tokens currently present in the vocabulary\n            current_tokens = set()\n            for w in vocab:\n                current_tokens.update(w.split())\n            \n            # Optional: Verbose log\n            # print(f\"Merged: {best_pair} -> {''.join(best_pair)}\")\n\n            # Stop if we have reached the target vocabulary size\n            # (Note: In this bottom-up logic, vocab usually shrinks as we merge chars, \n            # so we typically stop when no pairs are left or after N merges.\n            # For this demo, we rely on the 'pairs' check mostly.)\n            if len(current_tokens) >= self.vocab_size: \n                break\n\n        # 4. Finalize\n        sorted_tokens = sorted(list(current_tokens))\n        special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n        self.vocab = {t: i for i, t in enumerate(special_tokens + sorted_tokens)}\n        self.inv_vocab = {i: t for t, i in self.vocab.items()}\n        \n        print(\"--- Training Complete ---\")\n        print(f\"Final Vocab Size: {len(self.vocab)}\")\n\n    def encode(self, text):\n        \"\"\"\n        Encodes text using the 'MaxMatch' (Greedy Longest-Match) strategy.\n        \"\"\"\n        output_tokens = []\n        words = text.split() \n        \n        for word in words:\n            if word in self.vocab:\n                output_tokens.append(word)\n                continue\n            \n            chars = list(word)\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            \n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                \n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr \n                    \n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                \n                if cur_substr is None:\n                    is_bad = True\n                    break\n                \n                sub_tokens.append(cur_substr)\n                start = end\n            \n            if is_bad:\n                output_tokens.append(\"[UNK]\")\n            else:\n                output_tokens.extend(sub_tokens)\n                \n        return output_tokens\n\nif __name__ == \"__main__\":\n    # Test Data: Designed to force subword merges\n    data = [\n        \"deep learning models relate data to predictions\",\n        \"modeling unrelated concepts is difficult for simple models\",\n        \"the relation between tokens and embeddings is complex\",\n        \"we use tokenizers to handle related and unrelated words\",\n        \"relating these distinct concepts requires attention\",\n        \"tokenization is the first step in nlp models\"\n    ]\n    \n    # 1. Train\n    tokenizer = WordPiece(vocab_size=200) \n    tokenizer.train(data)\n    \n    # 2. Inspect Vocabulary\n    print(\"\\n--- Learned Vocabulary (Sample) ---\")\n    keys = list(tokenizer.vocab.keys())\n    # Show last 15 tokens (usually the most complex merged words)\n    print(keys[-15:])\n    \n    # 3. Test Inference\n    test_words = [\"unrelated\", \"modeling\", \"tokenization\"]\n    \n    print(\"\\n--- Inference Tests ---\")\n    for word in test_words:\n        encoded = tokenizer.encode(word)\n        print(f\"'{word}' -> {encoded}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:22:48.879998Z","iopub.execute_input":"2026-02-04T04:22:48.880900Z","iopub.status.idle":"2026-02-04T04:22:48.939113Z","shell.execute_reply.started":"2026-02-04T04:22:48.880868Z","shell.execute_reply":"2026-02-04T04:22:48.937942Z"}},"outputs":[{"name":"stdout","text":"--- Starting WordPiece Training (Target Vocab: 200) ---\nNo more pairs to merge. Stopping.\n--- Training Complete ---\nFinal Vocab Size: 42\n\n--- Learned Vocabulary (Sample) ---\n['relating', 'relation', 'requires', 'simple', 'step', 'the', 'these', 'to', 'tokenization', 'tokenizers', 'tokens', 'unrelated', 'use', 'we', 'words']\n\n--- Inference Tests ---\n'unrelated' -> ['unrelated']\n'modeling' -> ['modeling']\n'tokenization' -> ['tokenization']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}